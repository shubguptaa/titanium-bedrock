{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Why RAG is a Necessary Concept\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "Question Answering (QA) is an important task that involves extracting answers to factual queries posed in natural language. Typically, a QA system processes a query against a knowledge base containing structured or unstructured data and generates a response with accurate information. Ensuring high accuracy is key to developing a useful, reliable and trustworthy question answering system, especially for enterprise use cases. However, in this notebook, we will highlight a well documented issue with LLMs: LLM's are unable to answer questions outside of their training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27610c0f-7de6-4440-8f76-decf30e3c5ca",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup the `boto3` client connection to Amazon Bedrock\n",
    "\n",
    "Similar to notebook 00, we will create a client side connection to Amazon Bedrock with the `boto3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ae2b2a05-78a9-40ca-9b5e-121030f9ede1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "import logging\n",
    "import boto3\n",
    "\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "logging.basicConfig(level=logging.INFO,format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")\n",
    "claude3 = 'claude3'\n",
    "llama2 = 'llama2'\n",
    "llama3='llama3'\n",
    "mistral='mistral'\n",
    "titan='titan'\n",
    "models_dict = {\n",
    "    claude3 : 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "    llama2: 'meta.llama2-13b-chat-v1',\n",
    "    llama3: 'meta.llama3-8b-instruct-v1:0',\n",
    "    mistral: 'mistral.mistral-7b-instruct-v0:2',\n",
    "    titan : 'amazon.titan-text-premier-v1:0'\n",
    "}\n",
    "max_tokens_val = 200\n",
    "temperature_val = 0.1\n",
    "\n",
    "\n",
    "dict_add_params = {\n",
    "    llama3: {}, #\"max_gen_len\":max_tokens_val, \"temperature\":temperature_val} , \n",
    "    claude3: {\"top_k\": 200, },# \"temperature\": temperature_val, \"max_tokens\": max_tokens_val},\n",
    "    mistral: {}, #{\"max_tokens\":max_tokens_val, \"temperature\": temperature_val} , \n",
    "    titan:  {\"topK\": 200, },# \"maxTokenCount\": max_tokens_val}\n",
    "}\n",
    "inference_config={\n",
    "    \"temperature\": temperature_val,\n",
    "    \"maxTokens\": max_tokens_val,\n",
    "    \"topP\": 0.9\n",
    "}\n",
    "\n",
    "def generate_conversation(bedrock_client,model_id,system_text,input_text):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_text (JSON) : The system prompt.\n",
    "        input text : The input message.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Generating message with model %s\", model_id)\n",
    "\n",
    "    # Message to send.\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": input_text}]\n",
    "    }\n",
    "    messages = [message]\n",
    "    system_prompts = [{\"text\" : system_text}]\n",
    "\n",
    "    if model_id in [models_dict.get(mistral), models_dict.get(titan)]:\n",
    "        system_prompts = [] # not supported\n",
    "\n",
    "    # Inference parameters to use.\n",
    " \n",
    "\n",
    "    #Base inference parameters to use.\n",
    "    #inference_config = {\"temperature\": temperature}\n",
    "\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=get_additional_model_fields(model_id)\n",
    "    )\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_additional_model_fields(modelId):\n",
    "\n",
    "    return dict_add_params.get(modelId)\n",
    "    #{\"top_k\": top_k, \"max_tokens\": max_tokens}}\n",
    "    \n",
    "def get_converse_output(response_obj):\n",
    "    ret_messages=[]\n",
    "    output_message = response['output']['message']\n",
    "    role_out = output_message['role']\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        ret_messages.append(content['text'])\n",
    "        \n",
    "    return ret_messages, role_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7db128",
   "metadata": {},
   "source": [
    "#### Test an invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3cf9dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model meta.llama3-8b-instruct-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Title: The Devastating Impact of High Inflation on a Country's GDP\n",
       "\n",
       "As an economist, I have always been fascinated by the intricate relationship between inflation and a country's Gross Domestic Product (GDP). While a moderate level of inflation can be beneficial for economic growth, high inflation can have devastating consequences on a country's economy. In this article, I will explore the impact of high inflation on a country's GDP and provide evidence from various studies and data to support my arguments.\n",
       "\n",
       "What is High Inflation?\n",
       "\n",
       "High inflation is typically defined as an inflation rate above 5-6% per annum. When inflation rises above this threshold, it can lead to a decline in the purchasing power of consumers, reduced savings, and increased uncertainty in the economy. High inflation can also lead to a decrease in the value of money, making it more difficult for individuals and businesses to plan for the future.\n",
       "\n",
       "Impact on GDP\n",
       "\n",
       "High inflation can have a significant impact on a country's GDP in several ways"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "modelId = models_dict.get(llama3) #claude3) #llama3)\n",
    "system_text = \"You are an economist with access to lots of data.\"\n",
    "input_text = \"Write an article about impact of high inflation to GDP of a country.\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, input_text)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6832f0-92fa-44d0-a736-505f83890c7b",
   "metadata": {},
   "source": [
    "---\n",
    "## Highlighting the Contextual Issue\n",
    "\n",
    "We are trying to model a situation where we are asking the model to provide information about Amazon Advertizing Business. We will first ask the model based on the training data to provide us with an answer about pricing of this technoloy. This technique is called `Zero Shot`. Let's take a look at Claude's response to a quick question \"How did Amazon's Advertising business do?\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "29c6c4cd",
   "metadata": {},
   "source": [
    "Alongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% YoY from\n",
    "$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\n",
    "offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streaming\n",
    "TV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expanded\n",
    "our streaming TV advertising by introducing ads into Prime Video shows and movies, where brands can\n",
    "reach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies and\n",
    "shows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football. Streaming\n",
    "TV advertising is growing quickly and off to a strong start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51227cd9-fd1e-45e0-81ca-d5c9cd69d19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Unfortunately, I don't have access to Amazon's actual financial results for 2023 since that year is still in the future. As an AI assistant without direct connections to Amazon's internal data, I can only provide information based on what has been publicly reported so far.\n",
       "\n",
       "Amazon does have a large and growing advertising business as part of its overall operations, but specifics on its 2023 performance are not yet known. Major tech companies typically release their quarterly and annual financial reports a few weeks after the end of each period.\n",
       "\n",
       "If you're interested, I can share details on Amazon's advertising revenues and growth rates for past years based on the company's published reports and analyst estimates. However, for 2023 specifically, we'll have to wait until that data is released by Amazon next year. Let me know if you'd like me to provide historical advertising metrics for Amazon instead."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "modelId = models_dict.get(claude3) #claude3) #llama3)\n",
    "system_text = \"You are an economist with access to lots of data.\"\n",
    "input_text = \"How did Amazon's Advertising business do in 2023?\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, input_text)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6acd08-afcc-4f25-8f12-b4d146c7e5d1",
   "metadata": {},
   "source": [
    "The answer provided by Llama3 or Claude is actually incorrect based on Andy Jassi's letter to shareholder in 2023. This is not surprising because the letter is fairly new at the time of writing, meaning that there are more likely changes to the correct answer to the question which are not included in Claude's training data.\n",
    "\n",
    "This implies we need to augment the prompt with additional data about the desired technology question and then the model will return us a very factually accurate. We will see how this improves the response in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c92a2-6c28-48d0-9cd1-db93a7f9f1e2",
   "metadata": {},
   "source": [
    "---\n",
    "## Manually Providing Correct Context\n",
    "\n",
    "In order to have Claude correctly answer the question provided, we need to provide the model context which is relevant to the question. Below is a an extract from the letter to shareholders documentation. \n",
    "\n",
    "```\n",
    "Question:\n",
    "\n",
    "How did Amazon's Advertising business do in 2023?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Alongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% YoY from\n",
    "$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\n",
    "offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streaming\n",
    "TV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expanded\n",
    "our streaming TV advertising by introducing ads into Prime Video shows and movies, where brands can\n",
    "reach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies and\n",
    "shows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football. Streaming\n",
    "TV advertising is growing quickly and off to a strong start.\n",
    "```\n",
    "\n",
    "We can inject this context into the prompt as shown below and ask the LLM to answer our question based on the context provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09ab2ff0-4cce-43a4-bc55-84ab26e31980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "According to the context provided, Amazon's Advertising business performed strongly in 2023, growing 24% year-over-year from $38 billion in 2022 to $47 billion in 2023. This growth was primarily driven by Amazon's sponsored ads offerings.\n",
       "\n",
       "Some key points about Amazon's Advertising business in 2023:\n",
       "\n",
       "1. Revenue grew from $38 billion in 2022 to $47 billion in 2023, a 24% year-over-year increase.\n",
       "\n",
       "2. The growth was mainly fueled by Amazon's sponsored ads products.\n",
       "\n",
       "3. Amazon introduced a new offering called Sponsored TV, a self-service solution for brands to create ad campaigns across over 30 streaming TV services, including Amazon Freevee and Twitch.\n",
       "\n",
       "4. Amazon expanded streaming TV advertising by introducing ads into Prime Video shows and movies, allowing brands to reach over 200 million monthly viewers on Amazon's popular entertainment offerings.\n",
       "\n",
       "5. The streaming TV advertising segment is described as growing quickly and off to a strong start.\n",
       "\n",
       "Overall, the context suggests that Amazon's Advertising business, particularly its sponsored ads and streaming TV advertising offerings, experienced significant growth and momentum in 2023."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROMPT = '''Here is some important context which can help inform the questions the Human asks.\n",
    "\n",
    "<context> Amazon's Advertising business in 2023\n",
    "Alongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% YoY from\n",
    "$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\n",
    "offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streaming\n",
    "TV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expanded\n",
    "our streaming TV advertising by introducing ads into Prime Video shows and movies, where brands can\n",
    "reach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies and\n",
    "shows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football. Streaming\n",
    "TV advertising is growing quickly and off to a strong start.\n",
    "</context>\n",
    "\n",
    "Human: How did Amazon's Advertising business do in 2023?\n",
    "\n",
    "Assistant:\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "modelId = models_dict.get(claude3) #titan) #claude3) #llama3) mistral\n",
    "system_text = \"You are an economist with access to lots of data.\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, PROMPT)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79f602-fa67-42ce-b78b-2cde7ee96535",
   "metadata": {},
   "source": [
    "Now you can see that the model answers the question accurately based on the factual context. However, this context had to be added manually to the prompt. In a production setting, we need a way to automate the retrieval of this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fbe70",
   "metadata": {},
   "source": [
    "---\n",
    "## Quick Note: Long Context Windows\n",
    "\n",
    "One known limitation for RAG based solutions is the need for inclusion of lots of text into a prompt for an LLM. Fortunately, Claude can help this issue by providing an input token limit of 100k tokens. This limit [corresponds to around 75k words](https://www.anthropic.com/index/100k-context-windows) which is an astounding amount of text.\n",
    "\n",
    "Let's take a look at an example of Claude handling this large context size..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d98dc3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Great Gatsby By F. Scott Fitzgerald The Great Gatsby  ...\n",
      "The context contains 52854 words\n"
     ]
    }
   ],
   "source": [
    "book = ''\n",
    "with open('../data/book/book.txt', 'r') as f:\n",
    "    book = f.read()\n",
    "print('Context:', book[0:53], '...')\n",
    "print('The context contains', len(book.split(' ')), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95a8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is a summary of the plot of The Great Gatsby by F. Scott Fitzgerald:\n",
       "\n",
       "The story is narrated by Nick Carraway, who moves to New York to become a bond trader. He rents a house next door to a mysterious millionaire named Jay Gatsby. Nick is soon drawn into the wealthy Long Island social circle which includes his cousin Daisy and her brutish husband Tom Buchanan. \n",
       "\n",
       "Nick eventually learns that Gatsby is in love with Daisy and was her former lover before going off to war. Gatsby now hopes to win Daisy back with his newly acquired wealth and throws lavish parties in hopes she will attend. Nick arranges for Gatsby to reunite with Daisy, and they begin an affair.\n",
       "\n",
       "Tensions mount when Tom grows suspicious of Gatsby and Daisy's relationship. In a pivotal confrontation, Gatsby is exposed as a bootlegger who made his fortune through illegal means. Daisy is forced to choose between Gatsby and Tom, and after some hesitation, chooses to remain with her husband. \n",
       "\n",
       "Later that night, Daisy strikes and kills Tom's mistress Myrtle Wilson while driving Gatsby's car. Gatsby takes the blame for the accident. Myrtle's husband mistakes Gatsby for the driver and shoots him dead. Nick is left to contemplate the tragedy of Gatsby's romantic dream and obsession with wealth and status. In the end, Gatsby's quest for Daisy's love ends in ruin and death."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROMPT =f'''Human: Summarize the plot of this book.\n",
    "\n",
    "<book>\n",
    "{book}\n",
    "</book>\n",
    "\n",
    "Assistant:'''\n",
    "\n",
    "import json\n",
    "\n",
    "modelId = models_dict.get(claude3) #claude3) #llama3)\n",
    "system_text = \"You are a Literary scholar\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, PROMPT)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50a3d8a",
   "metadata": {},
   "source": [
    "#### Evaluation -\n",
    "\n",
    "So which model do we choose and how do we decide that . Let us look at some of the metrics\n",
    "\n",
    "![](./images/model_eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30cf6b3",
   "metadata": {},
   "source": [
    "#### Libraries available\n",
    "\n",
    "1. HuggingFace Evaluate\n",
    "2. FmEval - AWS\n",
    "3. RAGAs\n",
    ".........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "38b98f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model amazon.titan-text-premier-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Amazon's Advertising business grew 24% YoY from $38B in 2022 to $47B in 2023."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PROMPT = '''Here is some important context which can help inform the questions the Human asks.\n",
    "\n",
    "<context> Amazon's Advertising business in 2023\n",
    "Alongside our Stores business, Amazon’s Advertising progress remains strong, growing 24% YoY from\n",
    "$38B in 2022 to $47B in 2023, primarily driven by our sponsored ads. We’ve added Sponsored TV to this\n",
    "offering, a self-service solution for brands to create campaigns that can appear on up to 30+ streaming\n",
    "TV services, including Amazon Freevee and Twitch, and have no minimum spend. Recently, we’ve expanded\n",
    "our streaming TV advertising by introducing ads into Prime Video shows and movies, where brands can\n",
    "reach over 200 million monthly viewers in our most popular entertainment offerings, across hit movies and\n",
    "shows, award-winning Amazon MGM Originals, and live sports like Thursday Night Football. Streaming\n",
    "TV advertising is growing quickly and off to a strong start.\n",
    "</context>\n",
    "\n",
    "Human: How did Amazon's Advertising business do in 2023?\n",
    "'''\n",
    "\n",
    "import json\n",
    "\n",
    "modelId = models_dict.get(titan) #titan) #claude3) #llama3) mistral\n",
    "system_text = \"You are an economist with access to lots of data.\"\n",
    "response = generate_conversation(bedrock_runtime, modelId, system_text, PROMPT)\n",
    "output_message = response['output']['message']\n",
    "\n",
    "\n",
    "display(Markdown(get_converse_output(response)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0abfa9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = \"\"\"\n",
    "It grew 24% year-over-year from $38 billion in 2022 to $47 billion in 2023. It was driven primarily by Amazon's sponsored ads. Amazon also introduced Advertising on Prime Video. Amazon introduced self-service solutions for brands to create campaigns on up to 30+ streaming TV services.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e426506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating message with model amazon.titan-text-premier-v1:0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "INFO:__main__:Generating message with model meta.llama2-13b-chat-v1\n",
      "INFO:__main__:Generating message with model meta.llama3-8b-instruct-v1:0\n",
      "INFO:__main__:Generating message with model mistral.mistral-7b-instruct-v0:2\n",
      "INFO:__main__:Generating message with model anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "model_id-->titan, score -- > {'precision': [0.9213099479675293], 'recall': [0.8710747361183167], 'f1': [0.8954883813858032], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.39.3)'}::"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "model_id-->llama2, score -- > {'precision': [0.8608658909797668], 'recall': [0.9297766089439392], 'f1': [0.8939952850341797], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.39.3)'}::"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "model_id-->llama3, score -- > {'precision': [0.9240385890007019], 'recall': [0.8973273038864136], 'f1': [0.9104871153831482], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.39.3)'}::"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "model_id-->mistral, score -- > {'precision': [0.8877488374710083], 'recall': [0.9370240569114685], 'f1': [0.9117211699485779], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.39.3)'}::"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "model_id-->claude3, score -- > {'precision': [0.8633639812469482], 'recall': [0.9300545454025269], 'f1': [0.8954692482948303], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.39.3)'}::"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")\n",
    "predictions_list = []\n",
    "\n",
    "for modelId in [titan, llama2, llama3, mistral, claude3]:\n",
    "    \n",
    "    response = generate_conversation(bedrock_runtime, models_dict.get(modelId), system_text, PROMPT)\n",
    "    text_resp = get_converse_output(response)[0][0]\n",
    "    bert_score = bertscore.compute(predictions=[text_resp.replace(\"\\n\",\"\"),], references=[ground_truth,], lang=\"en\")\n",
    "\n",
    "    predictions_list.append( (modelId, bert_score, text_resp) ) # add as a tuple\n",
    "\n",
    "for score in predictions_list:\n",
    "    display(Markdown(f\"model_id-->{score[0]}, score -- > {score[1]}::\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now you have been able to see a concrete example where LLMs can be improved with correct context injected into a prompt, lets move on to notebook 02 to see how we can automate this process."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
